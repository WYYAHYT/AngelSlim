torchrun --nnodes=1 --nproc_per_node=8 train_llama.py \
--dataset_root "/path/to/dataset" \
--local_dir "/path/to/output" \
--model_path "/path/to/model" \
--model_family "llama" \
--dataset "ultrafineweb-10b" \
--dataset_format "pt" \
--output_model_filename "1B" \
--quant_method "sherry" \
--w_bits 0 \
--N 3 \
--M 4 \
--eps 1e-3 \
--granularity "per_group" \
--group_size 128 \
--num_train_epochs 1.0 \
--model_max_length 1024 \
--pt_context_len 1024 \
--source_max_len 1024 \
--target_max_len 1024 \
--do_train True \
--do_eval True \
--fp16 False \
--bf16 True \
--log_on_each_node False \
--logging_dir "../../output/Llama-3.2-1B" \
--per_device_train_batch_size 16 \
--per_device_eval_batch_size 16 \
--gradient_accumulation_steps 1 \
--save_strategy "steps" \
--save_steps 1000 \
--report_to "none" \
--save_total_limit 40 \
--learning_rate 1e-4 \
--weight_decay 0. \
--warmup_ratio 0.5 \
--lr_scheduler_type "constant" \
--logging_steps 1 \
--tf32 False \
--gradient_checkpointing False \
--qat True \

